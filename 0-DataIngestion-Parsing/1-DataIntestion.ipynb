{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a399ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5973fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f437aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26a790d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sample document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"This is a sample document.\",\n",
    "    metadata={\n",
    "        \"source\": \"sample.txt\",\n",
    "        \"author\": \"Darshan\",\n",
    "        \"date\": \"2023-01-01\",\n",
    "        \"tags\": [\"sample\", \"document\"],\n",
    "        \"url\": \"https://www.example.com/sample\",\n",
    "        \"length\": 100,\n",
    "        \"page\":1\n",
    "\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63837d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document main content This is a sample document.\n",
      "Document metadata {'source': 'sample.txt', 'author': 'Darshan', 'date': '2023-01-01', 'tags': ['sample', 'document'], 'url': 'https://www.example.com/sample', 'length': 100, 'page': 1}\n",
      "Document metadata Source sample.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document main content {doc.page_content}\")\n",
    "print(f\"Document metadata {doc.metadata}\")\n",
    "print(f\"Document metadata Source {doc.metadata['source']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c64232",
   "metadata": {},
   "source": [
    "#Reading text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93890176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64a50a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_files = {\n",
    "    \"data/text_files/models_in_langchain.txt\": \"\"\"LangChain Models: The Foundation of AI Applications\n",
    "\n",
    "LangChain provides a unified interface for working with various language models including OpenAI GPT, Anthropic Claude, Google PaLM, and Hugging Face models.\n",
    "The BaseLanguageModel class serves as the foundation for all model implementations in LangChain.\n",
    "Chat models like ChatOpenAI are designed for conversational interfaces and support system, human, and AI message types.\n",
    "Completion models like OpenAI are used for text completion tasks and single-turn interactions.\n",
    "Model parameters such as temperature, max_tokens, and top_p can be configured to control output randomness and length.\n",
    "LangChain supports both synchronous and asynchronous model calls for better performance in production applications.\n",
    "Model callbacks allow you to track token usage, latency, and costs across different model providers.\n",
    "The model abstraction enables easy switching between different providers without changing your application code.\n",
    "Streaming responses are supported for real-time applications where you need to display partial results.\n",
    "Model caching can be implemented to reduce API calls and improve response times for repeated queries.\n",
    "Custom models can be integrated by implementing the BaseLanguageModel interface.\n",
    "Batch processing capabilities allow you to process multiple inputs efficiently.\n",
    "Error handling and retry mechanisms are built into the model classes for robust production deployments.\n",
    "Model validation ensures that inputs meet the requirements of specific model providers.\n",
    "The LangChain model ecosystem continues to expand with new providers and capabilities being added regularly.\"\"\",\n",
    "    \n",
    "    \"data/text_files/chains_in_langchain.txt\": \"\"\"LangChain Chains: Orchestrating Complex AI Workflows\n",
    "\n",
    "Chains in LangChain are the building blocks for creating complex AI applications by linking multiple components together.\n",
    "The BaseChain class provides the foundation for all chain implementations with standardized input/output handling.\n",
    "LLMChain is the most basic chain that combines a language model with a prompt template for simple text generation tasks.\n",
    "SequentialChain allows you to chain multiple operations together where the output of one becomes the input of the next.\n",
    "SimpleSequentialChain is a simplified version for linear workflows with single input/output between steps.\n",
    "RouterChain enables conditional logic by routing inputs to different sub-chains based on content or criteria.\n",
    "TransformChain allows you to apply custom transformations to data as it flows through your pipeline.\n",
    "MapReduceChain is designed for processing large documents by splitting, processing in parallel, and combining results.\n",
    "StuffDocumentsChain combines multiple documents into a single prompt for processing by a language model.\n",
    "RefineDocumentsChain iteratively refines answers by processing documents one at a time and updating the response.\n",
    "MapRerankDocumentsChain processes documents independently and ranks the results to find the best answer.\n",
    "ConversationChain maintains context across multiple interactions for chatbot and conversational applications.\n",
    "SummarizationChain specializes in creating concise summaries from longer text documents.\n",
    "QAChain is optimized for question-answering tasks with built-in retrieval and response generation.\n",
    "Chain composition allows you to build complex workflows by combining simpler chains in sophisticated ways.\n",
    "Custom chains can be created by extending BaseChain and implementing the required methods for your specific use case.\"\"\",\n",
    "    \n",
    "    \"data/text_files/agents_in_langchain.txt\": \"\"\"LangChain Agents: Autonomous AI Decision Making\n",
    "\n",
    "Agents in LangChain are autonomous systems that can make decisions about which tools to use and how to use them.\n",
    "The Agent class combines a language model with a set of tools and decision-making logic to solve complex problems.\n",
    "ReAct agents use a reasoning and acting pattern to break down problems and execute solutions step by step.\n",
    "Zero-shot agents can work with tools they haven't seen before by using their descriptions and examples.\n",
    "Conversational agents maintain context across interactions while having access to tools and external resources.\n",
    "The AgentExecutor is responsible for running agents safely with proper error handling and execution limits.\n",
    "Tools are the external capabilities that agents can use, such as search engines, calculators, or API calls.\n",
    "Tool selection happens dynamically based on the agent's understanding of the current task and available options.\n",
    "Agent memory allows for persistence of information across multiple interactions and tool uses.\n",
    "Custom tools can be created by implementing the BaseTool interface with proper descriptions and input schemas.\n",
    "Agent callbacks provide visibility into the decision-making process and tool usage for debugging and monitoring.\n",
    "Multi-agent systems can be built where different agents specialize in different types of tasks or domains.\n",
    "Agent planning involves breaking down complex tasks into smaller, manageable steps that can be executed sequentially.\n",
    "Error recovery mechanisms help agents handle tool failures and unexpected situations gracefully.\n",
    "Agent evaluation frameworks help measure the performance and reliability of autonomous systems.\n",
    "Safety considerations include output filtering, tool access controls, and execution timeouts to prevent harmful actions.\"\"\",\n",
    "    \n",
    "    \"data/text_files/retrievers_in_langchain.txt\": \"\"\"LangChain Retrievers: Intelligent Information Retrieval\n",
    "\n",
    "Retrievers in LangChain are components designed to fetch relevant information from various data sources efficiently.\n",
    "The BaseRetriever class provides a standardized interface for all retrieval implementations in the framework.\n",
    "VectorStoreRetriever uses embedding-based similarity search to find relevant documents from vector databases.\n",
    "BM25Retriever implements the BM25 algorithm for keyword-based document retrieval with term frequency scoring.\n",
    "TFIDFRetriever uses Term Frequency-Inverse Document Frequency for traditional information retrieval tasks.\n",
    "SelfQueryRetriever can interpret natural language queries and convert them into structured database queries.\n",
    "MultiQueryRetriever generates multiple variations of a query to improve retrieval coverage and accuracy.\n",
    "EnsembleRetriever combines multiple retrieval methods to leverage the strengths of different approaches.\n",
    "ContextualCompressionRetriever filters and compresses retrieved documents to focus on the most relevant content.\n",
    "ParentDocumentRetriever maintains relationships between document chunks and their parent documents for better context.\n",
    "TimeWeightedVectorStoreRetriever considers both relevance and recency when ranking retrieved documents.\n",
    "MultiVectorRetriever can work with multiple vector representations of the same document for improved matching.\n",
    "Retrieval evaluation metrics help measure the quality and effectiveness of different retrieval strategies.\n",
    "Hybrid retrieval combines dense and sparse retrieval methods for optimal performance across different query types.\n",
    "Retrieval augmented generation (RAG) patterns use retrievers to provide context for language model responses.\n",
    "Custom retrievers can be implemented to work with proprietary data sources or specialized retrieval algorithms.\"\"\",\n",
    "    \n",
    "    \"data/text_files/embeddings_in_langchain.txt\": \"\"\"LangChain Embeddings: Vector Representations for AI\n",
    "\n",
    "Embeddings in LangChain convert text into high-dimensional vector representations that capture semantic meaning.\n",
    "The Embeddings base class provides a consistent interface for different embedding model providers and implementations.\n",
    "OpenAIEmbeddings integrates with OpenAI's text-embedding models for high-quality semantic representations.\n",
    "HuggingFaceEmbeddings allows you to use any embedding model from the Hugging Face model hub locally.\n",
    "SentenceTransformerEmbeddings specializes in creating embeddings optimized for semantic similarity tasks.\n",
    "CohereEmbeddings provides access to Cohere's multilingual embedding models for global applications.\n",
    "Embedding dimensions typically range from 384 to 1536 depending on the model and use case requirements.\n",
    "Batch embedding processing improves efficiency when working with large collections of documents.\n",
    "Embedding caching reduces computational costs by storing previously computed embeddings for reuse.\n",
    "Normalization of embeddings ensures consistent similarity calculations across different vector spaces.\n",
    "Embedding fine-tuning can improve performance for domain-specific applications and specialized vocabularies.\n",
    "Multilingual embeddings enable cross-language semantic search and similarity matching capabilities.\n",
    "Embedding evaluation involves measuring how well vectors capture semantic relationships in your specific domain.\n",
    "Vector databases store and index embeddings for fast similarity search and retrieval operations.\n",
    "Embedding visualization techniques help understand the semantic space and relationships between concepts.\n",
    "Custom embedding models can be integrated by implementing the Embeddings interface with proper tokenization and encoding.\"\"\",\n",
    "    \n",
    "    \"data/text_files/vectorstores_in_langchain.txt\": \"\"\"LangChain Vector Stores: Scalable Similarity Search\n",
    "\n",
    "Vector stores in LangChain provide efficient storage and retrieval of high-dimensional embeddings for similarity search.\n",
    "The VectorStore base class defines the standard interface for all vector database implementations.\n",
    "Chroma is a lightweight, open-source vector database that's perfect for development and small-scale applications.\n",
    "Pinecone offers a managed vector database service with high performance and scalability for production use.\n",
    "Weaviate provides a cloud-native vector database with built-in vectorization and hybrid search capabilities.\n",
    "FAISS (Facebook AI Similarity Search) enables efficient similarity search and clustering of dense vectors.\n",
    "Qdrant is a vector similarity search engine with extended filtering support and high availability features.\n",
    "Milvus is an open-source vector database built for scalable similarity search and AI applications.\n",
    "Vector store operations include adding documents, similarity search, and maximum marginal relevance retrieval.\n",
    "Metadata filtering allows you to combine vector similarity with traditional database-style filtering.\n",
    "Index types like IVF, HNSW, and LSH offer different trade-offs between speed, accuracy, and memory usage.\n",
    "Batch operations improve performance when adding or updating large numbers of documents in vector stores.\n",
    "Vector store persistence ensures that your embeddings and metadata are saved between application sessions.\n",
    "Hybrid search combines vector similarity with keyword search for more comprehensive retrieval capabilities.\n",
    "Vector store scaling involves considerations like sharding, replication, and distributed query processing.\n",
    "Custom vector stores can be implemented to integrate with proprietary or specialized similarity search systems.\"\"\",\n",
    "    \n",
    "    \"data/text_files/memory_in_langchain.txt\": \"\"\"LangChain Memory: Maintaining Context in AI Applications\n",
    "\n",
    "Memory components in LangChain enable AI applications to maintain context and state across multiple interactions.\n",
    "The BaseMemory class provides the foundation for all memory implementations with standardized read/write operations.\n",
    "ConversationBufferMemory stores the complete conversation history for maintaining full context in chat applications.\n",
    "ConversationBufferWindowMemory keeps only the last N interactions to manage memory usage in long conversations.\n",
    "ConversationSummaryMemory creates summaries of older conversations to maintain context while reducing memory footprint.\n",
    "ConversationSummaryBufferMemory combines recent messages with summaries of older interactions for optimal context management.\n",
    "EntityMemory tracks and maintains information about specific entities mentioned throughout the conversation.\n",
    "KnowledgeGraphMemory builds and maintains a knowledge graph of relationships between entities and concepts.\n",
    "VectorStoreRetrieverMemory uses similarity search to retrieve relevant past interactions based on current context.\n",
    "Memory persistence allows conversation state to be saved and restored across application sessions.\n",
    "Memory serialization enables memory state to be stored in databases or files for long-term persistence.\n",
    "Conversation memory can be shared across multiple chains or agents for consistent context management.\n",
    "Memory optimization techniques help balance context retention with computational and storage efficiency.\n",
    "Custom memory classes can be created to implement specialized context management for specific use cases.\n",
    "Memory evaluation involves measuring how well context is maintained and utilized across interactions.\n",
    "Memory security considerations include data privacy, access controls, and sensitive information handling.\"\"\",\n",
    "    \n",
    "    \"data/text_files/document_loaders_in_langchain.txt\": \"\"\"LangChain Document Loaders: Ingesting Data from Multiple Sources\n",
    "\n",
    "Document loaders in LangChain provide standardized ways to ingest data from various file formats and sources.\n",
    "The BaseLoader class defines the interface that all document loaders must implement for consistent behavior.\n",
    "TextLoader handles plain text files with configurable encoding and error handling for robust file processing.\n",
    "PDFLoader extracts text content from PDF documents while preserving structure and metadata information.\n",
    "CSVLoader processes comma-separated value files with customizable column handling and data type inference.\n",
    "JSONLoader parses JSON files and can extract specific fields or flatten nested structures as needed.\n",
    "WebBaseLoader scrapes content from web pages with support for custom selectors and content filtering.\n",
    "DirectoryLoader recursively processes multiple files in a directory with configurable file type filtering.\n",
    "UnstructuredLoader handles various document formats including Word, PowerPoint, and HTML files.\n",
    "NotionDBLoader connects to Notion databases to extract and synchronize content from collaborative workspaces.\n",
    "GitHubIssuesLoader retrieves issues and pull requests from GitHub repositories for analysis and processing.\n",
    "SlackDirectoryLoader extracts messages and conversations from Slack workspace exports.\n",
    "ConfluenceLoader connects to Atlassian Confluence to extract wiki pages and documentation.\n",
    "S3FileLoader and S3DirectoryLoader handle files stored in Amazon S3 buckets with proper authentication.\n",
    "Document metadata extraction preserves important information like creation dates, authors, and source locations.\n",
    "Custom loaders can be created by extending BaseLoader to handle proprietary formats or specialized data sources.\"\"\",\n",
    "    \n",
    "    \"data/text_files/text_splitters_in_langchain.txt\": \"\"\"LangChain Text Splitters: Intelligent Document Chunking\n",
    "\n",
    "Text splitters in LangChain break down large documents into smaller, manageable chunks for processing.\n",
    "The TextSplitter base class provides the foundation for all splitting strategies with configurable parameters.\n",
    "RecursiveCharacterTextSplitter intelligently splits text while trying to keep related content together.\n",
    "CharacterTextSplitter divides text based on specific characters like newlines or custom separators.\n",
    "TokenTextSplitter splits text based on token count to ensure chunks fit within model context limits.\n",
    "MarkdownHeaderTextSplitter preserves document structure by splitting on markdown headers and maintaining hierarchy.\n",
    "HTMLHeaderTextSplitter processes HTML documents while preserving semantic structure and tag information.\n",
    "CodeTextSplitter handles programming code with language-specific splitting that respects syntax boundaries.\n",
    "LatexTextSplitter processes LaTeX documents while maintaining mathematical expressions and document structure.\n",
    "PythonCodeTextSplitter specifically handles Python code with awareness of functions, classes, and logical blocks.\n",
    "Chunk size configuration balances context preservation with processing efficiency and model limitations.\n",
    "Chunk overlap ensures that important information spanning chunk boundaries is not lost during processing.\n",
    "Semantic splitting attempts to break text at natural boundaries like sentences or paragraphs for better coherence.\n",
    "Metadata preservation ensures that chunk-level information maintains references to source documents and locations.\n",
    "Splitting evaluation helps optimize chunk size and overlap parameters for specific use cases and document types.\n",
    "Custom splitters can be implemented to handle specialized document formats or domain-specific splitting requirements.\"\"\",\n",
    "    \n",
    "    \"data/text_files/output_parsers_in_langchain.txt\": \"\"\"LangChain Output Parsers: Structured Data from Language Models\n",
    "\n",
    "Output parsers in LangChain convert unstructured language model responses into structured, usable data formats.\n",
    "The BaseOutputParser class provides the foundation for all parsing implementations with validation and error handling.\n",
    "PydanticOutputParser uses Pydantic models to define expected output schemas and automatically parse responses.\n",
    "JSONOutputParser extracts and validates JSON data from language model responses with error recovery.\n",
    "ListOutputParser converts comma-separated or numbered lists into Python list objects for further processing.\n",
    "DatetimeOutputParser extracts and normalizes date and time information from natural language responses.\n",
    "EnumOutputParser restricts outputs to predefined choices and validates that responses match expected options.\n",
    "RegexParser uses regular expressions to extract specific patterns and information from model responses.\n",
    "StructuredOutputParser combines multiple parsing strategies to handle complex, multi-field responses.\n",
    "RetryOutputParser automatically retries parsing with corrected prompts when initial parsing fails.\n",
    "OutputFixingParser attempts to repair malformed outputs by using additional language model calls.\n",
    "CommaSeparatedListOutputParser specifically handles comma-delimited lists with proper escaping and trimming.\n",
    "BooleanOutputParser converts natural language responses into boolean values with configurable true/false indicators.\n",
    "Parser chaining allows multiple parsers to be combined for complex data extraction and transformation workflows.\n",
    "Validation schemas ensure that parsed outputs meet business rules and data quality requirements.\n",
    "Custom parsers can be created by extending BaseOutputParser to handle specialized output formats and validation rules.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b00c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, content in sample_text_files.items():\n",
    "    with open(file_path, \"w\" ,encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31605a17",
   "metadata": {},
   "source": [
    "#Text Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f7827d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\agents_in_langchain.txt'}, page_content=\"LangChain Agents: Autonomous AI Decision Making\\n\\nAgents in LangChain are autonomous systems that can make decisions about which tools to use and how to use them.\\nThe Agent class combines a language model with a set of tools and decision-making logic to solve complex problems.\\nReAct agents use a reasoning and acting pattern to break down problems and execute solutions step by step.\\nZero-shot agents can work with tools they haven't seen before by using their descriptions and examples.\\nConversational agents maintain context across interactions while having access to tools and external resources.\\nThe AgentExecutor is responsible for running agents safely with proper error handling and execution limits.\\nTools are the external capabilities that agents can use, such as search engines, calculators, or API calls.\\nTool selection happens dynamically based on the agent's understanding of the current task and available options.\\nAgent memory allows for persistence of information across multiple interactions and tool uses.\\nCustom tools can be created by implementing the BaseTool interface with proper descriptions and input schemas.\\nAgent callbacks provide visibility into the decision-making process and tool usage for debugging and monitoring.\\nMulti-agent systems can be built where different agents specialize in different types of tasks or domains.\\nAgent planning involves breaking down complex tasks into smaller, manageable steps that can be executed sequentially.\\nError recovery mechanisms help agents handle tool failures and unexpected situations gracefully.\\nAgent evaluation frameworks help measure the performance and reliability of autonomous systems.\\nSafety considerations include output filtering, tool access controls, and execution timeouts to prevent harmful actions.\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(r\"data\\text_files\\agents_in_langchain.txt\", encoding=\"utf-8\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "782b2664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 604.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Directory Loader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(r\"data\\text_files\", glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "\n",
    "docs = dir_loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8a27b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documnet number 1\n",
      "Document content: 1786\n",
      "Document metadata: data\\text_files\\agents_in_langchain.txt\n",
      "documnet number 2\n",
      "Document content: 1815\n",
      "Document metadata: data\\text_files\\chains_in_langchain.txt\n",
      "documnet number 3\n",
      "Document content: 1737\n",
      "Document metadata: data\\text_files\\document_loaders_in_langchain.txt\n",
      "documnet number 4\n",
      "Document content: 1748\n",
      "Document metadata: data\\text_files\\embeddings_in_langchain.txt\n",
      "documnet number 5\n",
      "Document content: 1818\n",
      "Document metadata: data\\text_files\\memory_in_langchain.txt\n",
      "documnet number 6\n",
      "Document content: 1639\n",
      "Document metadata: data\\text_files\\models_in_langchain.txt\n",
      "documnet number 7\n",
      "Document content: 1787\n",
      "Document metadata: data\\text_files\\output_parsers_in_langchain.txt\n",
      "documnet number 8\n",
      "Document content: 1820\n",
      "Document metadata: data\\text_files\\retrievers_in_langchain.txt\n",
      "documnet number 9\n",
      "Document content: 1801\n",
      "Document metadata: data\\text_files\\text_splitters_in_langchain.txt\n",
      "documnet number 10\n",
      "Document content: 1776\n",
      "Document metadata: data\\text_files\\vectorstores_in_langchain.txt\n"
     ]
    }
   ],
   "source": [
    "for i , doc in enumerate(docs):\n",
    "    print(f\"documnet number {i+1}\")\n",
    "    print(f\"Document content: {len(doc.page_content)}\")\n",
    "\n",
    "    print(f\"Document metadata: {doc.metadata['source']}\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26c8bc",
   "metadata": {},
   "source": [
    "#Text splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "653e583f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain Agents: Autonomous AI Decision Making\\n\\nAgents in LangChain are autonomous systems that can make decisions about which tools to use and how to use them.\\nThe Agent class combines a language model with a set of tools and decision-making logic to solve complex problems.\\nReAct agents use a reasoning and acting pattern to break down problems and execute solutions step by step.\\nZero-shot agents can work with tools they haven't seen before by using their descriptions and examples.\\nConversational agents maintain context across interactions while having access to tools and external resources.\\nThe AgentExecutor is responsible for running agents safely with proper error handling and execution limits.\\nTools are the external capabilities that agents can use, such as search engines, calculators, or API calls.\\nTool selection happens dynamically based on the agent's understanding of the current task and available options.\\nAgent memory allows for persistence of information across multiple interactions and tool uses.\\nCustom tools can be created by implementing the BaseTool interface with proper descriptions and input schemas.\\nAgent callbacks provide visibility into the decision-making process and tool usage for debugging and monitoring.\\nMulti-agent systems can be built where different agents specialize in different types of tasks or domains.\\nAgent planning involves breaking down complex tasks into smaller, manageable steps that can be executed sequentially.\\nError recovery mechanisms help agents handle tool failures and unexpected situations gracefully.\\nAgent evaluation frameworks help measure the performance and reliability of autonomous systems.\\nSafety considerations include output filtering, tool access controls, and execution timeouts to prevent harmful actions.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = docs[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54f19029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character chunks: 15\n",
      "Character chunks: The Agent class combines a language model with a set of tools and decision-making logic to solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "#character text splitter\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(text)\n",
    "print(f\"Character chunks: {len(char_chunks)}\")\n",
    "print(f\"Character chunks: {char_chunks[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "355980c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive character chunks: The Agent class combines a language model with a set of tools and decision-making logic to solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "#RecursiveCharacterTextSplitter \n",
    "\n",
    "rec_character_teext_splitter = RecursiveCharacterTextSplitter (\n",
    "    separators=[\"\\n\\n\", \"\\n\" , \" \", \"\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "rec_char_chunks = rec_character_teext_splitter.split_text(text)\n",
    "print(f\"Recursive character chunks: {rec_char_chunks[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57198f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
